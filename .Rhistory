remDr$open()
remDr$navigate("https://www.netimpact.org/blog/meet-niklaus-winner-of-the-nextgenmobility-challenge")
xpathExample <- remDr$findElement(using = "xpath","/html/body/div[2]/main/div[2]/div/div/div[2]/div[1]/div/h1")
xpathExample$getElementText()
remDr <- remoteDriver(browserName="firefox", port=4444)
library('RSelenium')
remDr <- remoteDriver(browserName="firefox", port=4444)
remDr$open()
results
remDr <- remoteDriver(browserName="firefox", port=4444)
remDr <- remoteDriver(browserName="firefox", port=4444)
library('RSelenium')
remDr <- remoteDriver(browserName="firefox", port=4444)
remDr$open()
remDr$navigate("http://property.phila.gov/")
addr <- "130 S 18th st."
unit <- "1001"
addressBox <- remDr$findElement('css selector','#search-address')
unitBox <- remDr$findElement('css selector','#search-unit')
addressBox$sendKeysToElement(list(addr))
unitBox$sendKeysToElement(list(unit))
unitBox$sendKeysToElement(list(key = 'enter'))
price_selector<- try(remDr$findElement('css selector','table > tbody > tr:nth-child(1) > td:nth-child(2)'))
class(price_selector)
value<-price_selector$getElementText()
value
price_selector
area_selector<-remDr$findElement('xpath','//*[@id="maincontent"]/div[3]/div[2]/div[2]/div[6]/div[2]/strong')
area_selector$getElementText()
area_text <- area$getElementText()
area_text
area_text <- area_selector$getElementText()
area_text
library(MASS)
library(reshape2)
library(ggplot2)
library(caret)
library(corrplot)
data(Boston)
names(Boston)
head(Boston)
inTrain <- createDataPartition(y = Boston$medv, p = .75, list = FALSE)
bostonTrain <- Boston[ inTrain,] #the new training set
bostonTest <- Boston[-inTrain,]  #the new test set
nrow(bostonTrain) #how many rows are in our new training set
fit <- lm(log(medv)~., bostonTrain)
summary(fit)
bostonTest$OLSpred <- exp(predict(fit, bostonTest))
plot(bostonTest$OLSpred,bostonTest$medv)
fitControl <- trainControl(method = "cv", number = 10)
set.seed(825)
olsFit <- train(log(medv) ~ ., data = Boston, method = "lm", trControl = fitControl)
olsFit
names(olsFit)
olsFit$resample
hist(olsFit$resample[,2], xlim=c(0,1))
fitControl <- trainControl(method = "cv", number = 50)
set.seed(825)
olsFit <- train(log(medv) ~ ., data = Boston, method = "lm", trControl = fitControl)
olsFit$resample
hist(olsFit$resample[,2], xlim=c(0,1))
fitControl <- trainControl(method = "cv", number = 10)
set.seed(825)
olsFit <- train(log(medv) ~ ., data = Boston, method = "lm", trControl = fitControl)
olsFit
names(olsFit)
olsFit$resample
hist(olsFit$resample[,2], xlim=c(0,1))
summary(olsFit)
plot(exp(predict(olsFit,Boston)), Boston$medv)
mean(abs((Boston$medv - exp(predict(olsFit,Boston)))/ Boston$medv))
fitControl <- trainControl(method = "cv", number = 10)
set.seed(825)
rfFit <- train(log(medv) ~ ., data = Boston, method = "rf", trControl = fitControl,importance=TRUE) #note importance
rfFit
hist(rfFit$resample[,2], xlim=c(0,1))
varImp(rfFit)
hist(rfFit$resample[,2], xlim=c(0,1))
varImp(rfFit)
plot(exp(predict(rfFit,Boston)), Boston$medv)
mean(abs((Boston$medv - exp(predict(rfFit,Boston)))/ Boston$medv))
fitControl <- trainControl(method = "cv", number = 10)
set.seed(825)
gbmFit <- train(log(medv) ~ ., data = Boston, method = "gbm", trControl = fitControl)
gbmFit
hist(gbmFit$resample[,2], xlim=c(0,1))
varImp(gbmFit)
plot(exp(predict(gbmFit,Boston)), Boston$medv)
mean(abs((Boston$medv - exp(predict(gbmFit,Boston)))/ Boston$medv))
olsResamples <- data.frame(cbind(model="OLS",GoodnessOfFit=olsFit$resample[,2]))
rfResamples <- data.frame(cbind(model="Random Forest",GoodnessOfFit=rfFit$resample[,2]))
gbmResamples <- data.frame(cbind(model="GBM",GoodnessOfFit=gbmFit$resample[,2]))
allResamples <- rbind(olsResamples,rfResamples,gbmResamples)
allResamples$GoodnessOfFit <- as.numeric(as.character(allResamples$GoodnessOfFit))
ggplot(allResamples, aes(x=GoodnessOfFit)) + geom_density(aes(group=model, colour=model, fill=model), alpha=0.3) + xlim(0,1)
names(gbmFit)
gbmfit$bestTune
gbmFit$bestTune
access_token <- "1647863048-oPZlbB3U0IeHktcSrDogiLeDgnmxsIYaQE0Xequ"
access_secret <-"n0Ije4XJLtAehrlWjJPSDPOAUZJMBkNQs5K0u3ih9Ve4n"
consumer_key <- "FHkW6ulKcOFlhy62EyDnwg4lA"
consumer_secret <- "R3iL9mJ4SwwMspwmPvBg6OhmW5lh26RBCy1qJNso2OUqsfpyDV"
oathfunction <- function(consumerKey, consumerSecret, accessToken, accessTokenSecret){
my_oauth <- ROAuth::OAuthFactory$new(consumerKey=consumerKey,
consumerSecret=consumerSecret,
oauthKey=accessToken,
oauthSecret=accessTokenSecret,
needsVerifier=FALSE, handshakeComplete=TRUE,
verifier="1",
requestURL="https://api.twitter.com/oauth/request_token",
authURL="https://api.twitter.com/oauth/authorize",
accessURL="https://api.twitter.com/oauth/access_token",
signMethod="HMAC")
return(my_oauth)
}
my_oauth <- oathfunction(consumer_key, consumer_secret, access_token, access_secret)
install.packages(tidytext)
install.packages('tidytext')
install.packages(syuzhet)
install.packages('syuzhet')
access_token <- "1647863048-oPZlbB3U0IeHktcSrDogiLeDgnmxsIYaQE0Xequ"
access_secret <-"n0Ije4XJLtAehrlWjJPSDPOAUZJMBkNQs5K0u3ih9Ve4n"
consumer_key <- "FHkW6ulKcOFlhy62EyDnwg4lA"
consumer_secret <- "R3iL9mJ4SwwMspwmPvBg6OhmW5lh26RBCy1qJNso2OUqsfpyDV"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
trump_tweets <- userTimeline("realDonaldTrump", n = 3200)
library(dplyr)
library(tidyr)
library(ROAuth) #Twitter api authorization
library(streamR) #Streaming api
library(twitteR) #Rest api
library(lubridate) #functions for date/time data
library(scales) #scales for data visualization
library(ggplot2)
library(stringr) #string manipulation
library(tidytext) #for text mining
library(syuzhet) #corpus
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
trump_tweets <- userTimeline("realDonaldTrump", n = 3200)
trumpDf <- twListToDF(trump_tweets)
tweets <- trumpDf %>%
select(id, statusSource, text, created) %>%
extract(statusSource, "source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
tweets %>%
count(source, hour = hour(with_tz(created, "EST"))) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
View(trumpDf)
tweets %>%
count(source, hour = hour(with_tz(created, "EST"))) %>%
mutate(percent = n / sum(n)) %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)", y = "% of tweets", color = "")
tweets %>%
count(source, quoted = ifelse(str_detect(text, '^"'), "Quoted", "Not quoted")) %>%
ggplot(aes(source, n, fill = quoted)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "") +
ggtitle('Whether tweets start with a quotation mark (")')
tweet_picture_counts <- tweets %>%
filter(!str_detect(text, '^"')) %>%
count(source, picture = ifelse(str_detect(text, "t.co"), "Picture/link", "No picture/link"))
ggplot(tweet_picture_counts, aes(source, n, fill = picture)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets %>%
filter(!str_detect(text, '^"')) %>%    #does the tweet begin with a quote
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
tweet_words %>%
filter(source == 'iPhone') %>%
count(word, sort = TRUE) %>%
head(20) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity") +
ylab("Occurrences") +
coord_flip()
tweet_words <- tweets %>%
filter(!str_detect(text, '^"')) %>%    #does the tweet begin with a quote
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
tweet_words %>%
filter(source == 'iPhone') %>%
count(word, sort = TRUE) %>%
head(20) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity") +
ylab("Occurrences") +
coord_flip()
tweet_words %>%
filter(source == 'Android') %>%
count(word, sort = TRUE) %>%
head(20) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat = "identity") +
ylab("Occurrences") +
coord_flip()
android_iphone_ratios <- tweet_words %>%
count(word, source) %>%
filter(sum(n) >= 5) %>%
spread(source, n, fill = 0) %>%
ungroup() %>%
mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
mutate(logratio = log2(Android / iPhone)) %>%
arrange(desc(logratio))
android_iphone_ratios %>%
group_by(logratio > 0) %>%
top_n(15, abs(logratio)) %>%
ungroup() %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
geom_bar(stat = "identity") +
coord_flip() +
ylab("Android / iPhone log ratio") +
scale_fill_manual(name = "", labels = c("Android", "iPhone"),
values = c("red", "lightblue"))
iPhoneTweets <- count(tweet_words,word, source) %>%
filter(source == 'iPhone')
androidTweets <- count(tweet_words,word, source) %>%
filter(source == 'Android')
iPhoneSentiment <- get_nrc_sentiment(iPhoneTweets$word)
get_nrc_sentiment("i dislike penn")
get_sentiment("i dislike penn")
get_sentiment("i dislike penn", method = 'nrc')
get_sentiment("i hate penn", method = 'nrc')
get_sentiment("i hate trump", method = 'nrc')
get_sentiment("i hate the snow", method = 'nrc')
get_sentiment("i love the snow", method = 'nrc')
get_sentiment("i like the snow", method = 'nrc')
get_sentiment("i don't care about the snow", method = 'nrc')
iPhoneSentiment <- get_nrc_sentiment(iPhoneTweets$word)
androidSentiment <- get_nrc_sentiment(androidTweets$word)
summarise_each(iPhoneSentiment, funs(mean))
summarise_each(androidSentiment, funs(mean))
summarise(sentiment, anger=mean(anger), anticipation=mean(anticipation), anger=mean(disgust), anticipation=mean(anticipation))
?count
iPhoneTweets
summarise(iPhoneSentiment, anger=mean(anger), anticipation=mean(anticipation), anger=mean(disgust), anticipation=mean(anticipation))
summarise(androidSentiment, anger=mean(anger), anticipation=mean(anticipation), anger=mean(disgust), anticipation=mean(anticipation))
library(SpatialEpiApp)
run_app()
install.packages('SpatialEpiApp')
runApp()
library(SpatialEpiApp)
run_app()
install.packages('INLA')
library(SpatialEpiApp)
run_app()
library(socrata)
library(RSocrata)
read.socrata(url, app_token = NULL, email = NULL, password = NULL,
stringsAsFactors = FALSE)
read.socrata('https://data.cityofmadison.com/resource/529s-j4ei.json', app_token = NULL, email = NULL, password = NULL,
stringsAsFactors = FALSE)
setwd("~/2016_UPennMUSA/04_Courses/08 MUSA 620/FinalProj")
wtire.csv('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', file='test.csv')
write.csv('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', file='test.csv')
download.file('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', file='test.csv')
download.file('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', destfile='test.csv')
read.socrata('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', app_token = NULL, email = NULL, password = NULL,
stringsAsFactors = FALSE)
read.socrata('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', app_token = NULL, email = NULL, password = NULL)
download.file('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', destfile='2016_Capital_City.csv')
download.file('https://data.cityofmadison.com/api/views/s42u-ebka/rows.csv?accessType=DOWNLOAD', destfile = '2016_Monroe.csv')
capital <- download.file('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', destfile='2016_Capital_City.csv')
monroe <- download.file('https://data.cityofmadison.com/api/views/s42u-ebka/rows.csv?accessType=DOWNLOAD', destfile = '2016_Monroe.csv')
head(capital)
?read.csv
capital <- read.csv('2016_Capital_City.csv')
head(capital)
monroe <- read.csv('2016_Monroe.csv')
head(monroe)
head(capital, 20)
capital$Eco.Totem...Capital.City.Trail...N..Shore.Dr. == capital$Channel.1.IN..Eastbound. + capital$Channel.2.OUT..Westbound.
sum(capital$Eco.Totem...Capital.City.Trail...N..Shore.Dr. == capital$Channel.1.IN..Eastbound. + capital$Channel.2.OUT..Westbound.)
class(capital$Date)
library(dplyr)
library(lubridate)
library(readr)
library(magrittr)
library(stringr)
library(purrr)
?as.POSIXlt
capital$Date %<>%
as.POSIXlt(format = "%Y%m%d %H:%M:%S")
class(capital$Date)
head(capital$Date,20)
capital <- read.csv('2016_Capital_City.csv')
head(capital, 20)
capital$Date %<>%
as.POSIXlt(format = "%Y/%m/%d %H:%M:%S")
head(capital$Date,20)
capital <- read.csv('2016_Capital_City.csv')
?ymd_hms
capital$Date %<>%ymd_hms()
head(capital$Date,20)
capital <- read.csv('2016_Capital_City.csv')
capital$Date %<>%ymd_hms()
capital <- read.csv('2016_Capital_City.csv')
head(capital)
capital$Date %<>%ymd_hms(as.character())
head(capital$Date,20)
capital <- read.csv('2016_Capital_City.csv')
capital$Date %<>%dmy_hms(as.character())
head(capital$Date,20)
capital$Date[35000]
capital <- read.csv('2016_Capital_City.csv')
capital$Date[35000]
capital$Date %<>%strptime(as.character())
capital$Date %<>% strptime(as.character(),format="%m/%d/%Y %I:%M:%S %p")
capital$Date %<>% strptime(as.character(),format="%m/%d/%Y %I:%M:%S %p",tz="CST")
capital <- read.csv('2016_Capital_City.csv')
capital$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p",tz="CST")
head(capital$Date,20)
capital <- read.csv('2016_Capital_City.csv')
capital$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p")
head(capital$Date,20)
capital <- read.csv('2016_Capital_City.csv')
capital$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p",tz="CST6CDT")
head(capital$Date,20)
capital$Date[35000]
monroe$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p",tz='CST6CDT')
monroe$Date[30000]
monroe$Date[35100]
head(capital$Date,20)
head(capital,20)
Sum <- aggregate(capital$Eco.Totem...Capital.City.Trail...N..Shore.Dr.,
list(hour=cut(as.POSIXct(capital$Date), "hour")),
sum)
head(Sum,30)
my.time <- seq(from = as.POSIXct('2000-01-01 00:00:00'),
to = as.POSIXct('2000-01-01 2:00:00'),
by = '10 min')
my.data <- rep(10, length = length(my.time))
my.data <- as.xts(my.data, order.by = my.time)
apply.hourly(my.data, sum)
library(xts)
my.time <- seq(from = as.POSIXct('2000-01-01 00:00:00'),
to = as.POSIXct('2000-01-01 2:00:00'),
by = '10 min')
my.data <- rep(10, length = length(my.time))
my.data <- as.xts(my.data, order.by = my.time)
apply.hourly(my.data, sum)
my.data
apply.hourly <- function(x, FUN,...) {
ep <- endpoints(x, 'hours')
period.apply(x, ep, FUN, ...)
}
my.time <- seq(from = as.POSIXct('2000-01-01 00:00:00'),
to = as.POSIXct('2000-01-01 2:00:00'),
by = '10 min')
my.data <- rep(10, length = length(my.time))
my.data <- as.xts(my.data, order.by = my.time)
apply.hourly(my.data, sum)
?endpoints
head(Sum,30)
capital <- aggregate(capital$Eco.Totem...Capital.City.Trail...N..Shore.Dr.,
list(hour=cut(as.POSIXct(capital$Date), "hour")),
sum)
capital <- read.csv('2016_Capital_City.csv')
monroe <- read.csv('2016_Monroe.csv')
capital$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p",tz="CST6CDT")
monroe$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p",tz='CST6CDT')
capital_hourly <- aggregate(capital$Eco.Totem...Capital.City.Trail...N..Shore.Dr.,
list(hour=cut(as.POSIXct(capital$Date), "hour")),
sum)
monroe_hourly <- aggregate(monroe$Eco.Totem...SW.Path...Monroe.St,
list(hour=cut(as.POSIXct(monroe$Date), "hour")),
sum)
getwd()
saveRDS(capital_hourly, "capital_hourly.rds")
saveRDS(monroe_hourly,"monroe_hourly.rds")
runApp('MadisonBikeCount')
runApp('~/2016_UPennMUSA/04_Courses/08 MUSA 620/HW4-Shiny')
?reactive
runApp('MadisonBikeCount')
runApp('MadisonBikeCount')
runApp('MadisonBikeCount')
runApp('MadisonBikeCount')
runApp('MadisonBikeCount')
runApp('~/2016_UPennMUSA/04_Courses/08 MUSA 620/musa620-seanKross/week12/leaflet-app')
runApp('MadisonBikeCount')
head(capital_hourly)
runApp('MadisonBikeCount')
runApp('MadisonBikeCount')
runApp('MadisonBikeCount')
runApp('MadisonBikeCount')
hour_of_dat <- hour(capital_hourly$hour)
head(hour_of_dat,40)
capital_hourly$hour_of_dat <- hour(capital_hourly$hour)
capital_groupByHour <- group_by(capital_hourly,hour_of_dat)
head(capital_groupByHour)
capital_groupByHour <- summarise(capital_groupByHour,sumByHour = sum(x))
capital_groupByHour
head(capital_groupByHour, 24)
head(capital_groupByHour, 24)
ggplot(capital_groupByHour)
ggplot(capital_groupByHour) + geom_bar()
ggplot(capital_groupByHour) + geom_bar(stat_count(identity))
?mutate
?month
head(month(capital_hourly))
head(month(capital_hourly$hour))
unique(month(capital_hourly$hour))
capital_groupByMonth <- capital_hourly %>%
mutate(monthOfYear = month(capital_hourly$hour)) %>%
group_by(monthOfYear) %>%
summarise(sumByMonth = sum(x))
capital_groupByMonth
?wday
capital_groupByWday <- capital_hourly %>%
mutate(dayOfWeek = wday(capital_hourly$hour, label=TRUE, abbr = TRUE)) %>%
group_by(dayOfWeek) %>%
summarise(sumByWday = sum(x))
capital_groupByWday
sum(capital_groupByHour$x)
sum(capital_groupByHour$sumByHour)
sum(capital_groupByMonth$sumByMonth)
sum(capital_groupByWday$sumByWday)
saveRDS(capital_groupByHour,"capital_groupByHour.rds")
saveRDS(capital_groupByMonth,"capital_groupByMonth.rds")
getwd
getwd()
setwd("C:/Users/Kristen Zhao/Documents/2016_UPennMUSA/04_Courses/08 MUSA 620/FinalProj/MadisonBikeCount"
)
setwd("C:/Users/Kristen Zhao/Documents/2016_UPennMUSA/04_Courses/08 MUSA 620/FinalProj/MadisonBikeCount"0
#library(RSocrata)
# in https://data.cityofmadison.com/Parks-Recreation/2016-Bike-Counter-Data-Capital-City-Trail/9jyv-r62h, click
# download tab, then right click on csv tab, copy link address, then paste below.
download.file('https://data.cityofmadison.com/api/views/9jyv-r62h/rows.csv?accessType=DOWNLOAD', destfile='2016_Capital_City.csv')
download.file('https://data.cityofmadison.com/api/views/s42u-ebka/rows.csv?accessType=DOWNLOAD', destfile = '2016_Monroe.csv')
capital <- read.csv('2016_Capital_City.csv')
head(capital)
#sum(capital$Eco.Totem...Capital.City.Trail...N..Shore.Dr. == capital$Channel.1.IN..Eastbound. + capital$Channel.2.OUT..Westbound.)
monroe <- read.csv('2016_Monroe.csv')
head(monroe)
class(capital$Date)
capital$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p",tz="CST6CDT")
monroe$Date %<>% strptime(format="%m/%d/%Y %I:%M:%S %p",tz='CST6CDT')
## aggregate data based on hour
capital_hourly <- aggregate(capital$Eco.Totem...Capital.City.Trail...N..Shore.Dr.,
list(hour=cut(as.POSIXct(capital$Date), "hour")),
sum)
monroe_hourly <- aggregate(monroe$Eco.Totem...SW.Path...Monroe.St,
list(hour=cut(as.POSIXct(monroe$Date), "hour")),
sum)
# save datasets as RDS files
saveRDS(capital_hourly, "capital_hourly.rds")
saveRDS(monroe_hourly,"monroe_hourly.rds")
# data shaping
# 1. Capital City Trail Dataset
## group data by hour
capital_hourly$hour_of_dat <- hour(capital_hourly$hour)
capital_groupByHour <- group_by(capital_hourly,hour_of_dat)
capital_groupByHour <- summarise(capital_groupByHour,sumByHour = sum(x))
capital_groupByHour
head(capital_groupByHour, 24)
head(hour_of_dat,40)
saveRDS(capital_groupByHour,"capital_groupByHour.rds")
## group by month
capital_groupByMonth <- capital_hourly %>%
mutate(monthOfYear = month(capital_hourly$hour)) %>%
group_by(monthOfYear) %>%
summarise(sumByMonth = sum(x))
capital_groupByMonth
saveRDS(capital_groupByMonth,"capital_groupByMonth.rds")
## group by day of week
capital_groupByWday <- capital_hourly %>%
mutate(dayOfWeek = wday(capital_hourly$hour, label=TRUE, abbr = TRUE)) %>%
group_by(dayOfWeek) %>%
summarise(sumByWday = sum(x))
capital_groupByWday
sum(capital_groupByHour$sumByHour)
sum(capital_groupByMonth$sumByMonth)
sum(capital_groupByWday$sumByWday)
setwd("C:/Users/Kristen Zhao/Documents/2016_UPennMUSA/04_Courses/08 MUSA 620/FinalProj/MadisonBikeCount")
saveRDS(capital_groupByWday,"capital_groupByWday.rds")
monroe_groupByHour <- monroe_hourly %>%
mutate(hourOfDay = hour(monroe_hourly$hour)) %>%
group_by(hourOfDay) %>%
summarise(sumByHour = sum(x))
monroe_groupByHour
saveRDS(monroe_groupByHour,"monroe_groupByHour.rds")
monroe_groupByMonth <- monroe_hourly %>%
mutate(monthOfYear = month(monroe_hourly$hour)) %>%
group_by(monthOfYear) %>%
summarise(sumByMonth = sum(x))
monroe_groupByMonth
saveRDS(monroe_groupByMonth,"monroe_groupByMonth.rds")
monroe_groupByWday <- monroe_hourly %>%
mutate(dayOfWeek = wday(monroe_hourly$hour, label=TRUE, abbr = TRUE)) %>%
group_by(dayOfWeek) %>%
summarise(sumByWday = sum(x))
saveRDS(monroe_groupByWday,"monroe_groupByWday.rds")
sum(capital_groupByHour$sumByHour)
sum(capital_groupByMonth$sumByMonth)
sum(capital_groupByWday$sumByWday)
sum(monroe_groupByHour$sumByHour)
sum(monroe_groupByMonth$sumByMonth)
sum(monroe_groupByWday$sumByWday)
